[toc]
# 机器学习(ML)
T:任务  
E:经验  
性能度量：P  
## 监督学习
人为的对数据有满足任务目标的定性，类似这个目标问题的‘正确答案’（手动完成标记），比如对数据的分类和具体的值  
监督学习常见的分为：  
1. 回归问题：处理连续值  
2. 分类问题：处理间断值  
## 无监督学习（聚类算法）
缺少人为的标记，由算法识别数据内部间的模式与关系。
# 单变量线性回归
## 模型描述  
m = 训练样本的数量  
x = 输入变量（特征）  
y = 输出变量（特征）  
$(x,y)$  一个训练样本  
$(x^{i},y^{i})$ 第i个训练样本
> 监督学习算法的工作流程：  
> ![监督学习算法示意图](/images/image.png)
## 单变量线性回归模型
假设函数：  
$$h_\theta(x)=\theta_0+\theta_1\cdot x$$  
$\theta_i$称为模型参数
### 均方误差代价函数
> 代价函数：  
> 任何能够衡量模型预测出来的值h(θ)与真实值y之间的差异的函数都可以叫做代价函数C(θ)，如果有多个样本，则可以将所有代价函数的取值求均值，记作J(θ)  
> - 对于每种算法来说，代价函数不是唯一的；
> - 代价函数是参数θ的用函数
> - 总的代价函数J(θ)可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本(x,y)；
> - J(θ)是一个标量；  
>理想情况下，当我们取到代价函数J的最小值时，就得到了最优的参数θ  

代价函数：  
$J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits_{i=1}^{m}\left( h_\theta(x)-y\right)^2   $  
优化目标：  
$minimizeJ(\theta_0,\theta_1)$

### 梯度下降法
从给定的点出发，按照一定的学习速率尝试下降速率最大的点，得到局部（或全局）的最小值点  
以代价函数为例：  
$$\theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,\dots)$$  
:= 表示赋值  
$\alpha$ 表示下降学习速率（步长）  
$J(\theta)$ 表示代价函数，$\theta$为参数，可以有多个
在循环到收敛的过程中，要实现多个参数的同步更新


# 多变量线性回归
![alt text](\images\image2.png)
n = 变量个数  
m = 样本个数  
$$x^i=\begin{bmatrix}
    x_1^i \\
    x_2^i \\
    \dots
\end{bmatrix}$$  
$x^i$ 表示第i个训练样本  
$x^i_j$ 表示第i个训练样本下的变量值  
## 假设函数
假设函数：  
$$h_\theta(x)=\theta_0+\theta_1\cdot x_1+\theta_2 \cdot x_2+\theta_3\cdot x_3+\dots$$   
也可以写作：  
变量向量$x=\begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2 \\
    x_3 \\
    \dots
\end{bmatrix}\quad(x_0=1)$  
参数向量$\theta = \begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
    \theta_2 \\
    \theta_3 \\
    \dots
\end{bmatrix}$   
$$h_\theta(x) = \theta^T\cdot x $$  

## 代价函数
同一元的回归方程代价函数  
$$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}\left(h_\theta(x^i)-y\right)^2$$
多元代价函数使用梯度下降法求局部最小值：  
$$ \theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta)$$
注意变量$\theta$同时更新  

