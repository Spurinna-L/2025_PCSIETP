[toc]
# 机器学习(ML)
T:任务  
E:经验  
性能度量：P  
## 监督学习
人为的对数据有满足任务目标的定性，类似这个目标问题的‘正确答案’（手动完成标记），比如对数据的分类和具体的值  
监督学习常见的分为：  
1. 回归问题：处理连续值  
2. 分类问题：处理间断值  
## 无监督学习（聚类算法）
缺少人为的标记，由算法识别数据内部间的模式与关系。
# 单变量线性回归
## 模型描述  
m = 训练样本的数量  
x = 输入变量（特征）  
y = 输出变量（特征）  
$(x,y)$  一个训练样本  
$(x^{i},y^{i})$ 第i个训练样本
> 监督学习算法的工作流程：  
> ![监督学习算法示意图](/images/machine_learning/image.png)
## 单变量线性回归模型
假设函数：  
$$h_\theta(x)=\theta_0+\theta_1\cdot x$$  
$\theta_i$称为模型参数
### 均方误差代价函数
> 代价函数：  
> 任何能够衡量模型预测出来的值h(θ)与真实值y之间的差异的函数都可以叫做代价函数C(θ)，如果有多个样本，则可以将所有代价函数的取值求均值，记作J(θ)  
> - 对于每种算法来说，代价函数不是唯一的；
> - 代价函数是参数θ的用函数
> - 总的代价函数J(θ)可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本(x,y)；
> - J(θ)是一个标量；  
>理想情况下，当我们取到代价函数J的最小值时，就得到了最优的参数θ  

代价函数：  
$J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits_{i=1}^{m}\left( h_\theta(x)-y\right)^2   $  
优化目标：  
$minimizeJ(\theta_0,\theta_1)$

### 梯度下降法
从给定的点出发，按照一定的学习速率尝试下降速率最大的点，得到局部（或全局）的最小值点  
以代价函数为例：  
$$\theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,\dots)$$  
:= 表示赋值  
$\alpha$ 表示下降学习速率（步长）  
$J(\theta)$ 表示代价函数，$\theta$为参数，可以有多个
在循环到收敛的过程中，要实现多个参数的同步更新


# 多变量线性回归
![多变量线性回归模型描述](\images\machine_learning\image2.png)
n = 变量个数  
m = 样本个数  
$$x^i=\begin{bmatrix}
    x_1^i \\
    x_2^i \\
    \dots
\end{bmatrix}$$  
$x^i$ 表示第i个训练样本  
$x^i_j$ 表示第i个训练样本下的变量值  
## 假设函数
假设函数：  
$$h_\theta(x)=\theta_0+\theta_1\cdot x_1+\theta_2 \cdot x_2+\theta_3\cdot x_3+\dots$$   
也可以写作：  
变量向量$x=\begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2 \\
    x_3 \\
    \dots
\end{bmatrix}\quad(x_0=1)$  
参数向量$\theta = \begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
    \theta_2 \\
    \theta_3 \\
    \dots
\end{bmatrix}$   
$$h_\theta(x) = \theta^T\cdot x $$  

## 代价函数和梯度下降
同一元的回归方程代价函数  
$$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}\left(h_\theta(x^i)-y\right)^2$$
多元代价函数使用梯度下降法求局部最小值：  
$$ \theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta)$$
注意变量$\theta$同时更新  

### 特征缩放  

当不同特征变量差异很大时，可以考虑使用特征放缩和均值归一化的方法加快运算  
均值归一化：  
$$ x_i' = \frac{x_i-\mu_i}{max(x_i)-min(x_i)}$$  
$\mu_i$是$x_i$的均值
> 也就是把多个变量均值统一为0
  
特征放缩：  
$$x' =\frac{x_i}{max(x_i)-min(x_i)} $$
> 也就是把多个变量的定义域统一到(-1,1)
​
### 调参 和 学习率$\alpha$
如果$\alpha$过小，则收敛速率很慢  
如果$\alpha$过大，则有可能不收敛或者可能出现异常的代价函数J增加  

- 通常通过绘制J和迭代次数的点线图像来判断是否收敛以及确定收敛点    
- 也可以设定阈值$\epsilon$，当$J(\theta)<\epsilon$时，使用此时的收敛点（不太推荐）
## 新特征和多项式回归
![多项式回归](\images\machine_learning\image3.png)
当样本不符合线性关系时，可以考虑使用多项式回归，选取新的特征变量，按照相同的模式：  
> 构建代价函数 -> 梯度下降法选取合适参数

## 正规方程
仅能适用于多元线性回归
![正规方程求解代价函数最小值点](\images\machine_learning\image4.png)
X为设计矩阵，y为因变量矩阵  
设计矩阵：  
若x为一个特征矩阵 $x^i=\begin{bmatrix}
    x_0^i \\
    x_1^i \\
    \dots
\end{bmatrix}$  
设计矩阵X为多个x的转置组成的矩阵
取$X=\begin{bmatrix}
    (x^1)^T \\
    (x^2)^T \\
    \dots   \\
    (x^m)^T    
\end{bmatrix}$
> 使用正规方程求参数时，不需要均值归一化和特征缩放  

# 分类问题
## logistic(sigmoid)回归算法（分类算法）
### 假设函数
在线性回归的基础上，对假设函数进行逻辑处理
> Logistic 回归的本质就是：  
> 对一个线性回归表达式的结果，套一个 sigmoid 函数，把输出从实数域压缩到(0,1)

$$h_\theta(x) = g(\theta^T\cdot x) = \frac{1}{1+e^{-\theta^T\cdot x}}$$

$\theta = \begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
    \theta_2 \\
    \dots
\end{bmatrix}\quad\quad$
$x = \begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2 \\
    \dots
\end{bmatrix}(x_0 = 1)$
![分类算法](\images\machine_learning\image5.png)
### 决策界限
用0.5作为$h_\theta(x)$的阈值，分类结果y满足：
$$\begin{equation*}
y = \begin{cases}
0 & \text{if } h_\theta(x) < 0 \\
1 & \text{if } h_\theta(x) \geq 0
\end{cases}
\end{equation*}$$
那么当$h_\theta(x)=0$时，即为该假设函数的决策界限  
一侧取0，另一侧取1  
### 代价函数$J(\theta)$
定义$J(\theta) = \frac{1}{m}\sum cost(h_\theta(x),y)$  
$$cost(h_\theta(x)^i,y^i)=\begin{cases}
    &-log(h_\theta(x)^i) \quad& if & y=1 \\
    &-log(1-h_\theta(x)^i)\quad & if & y=0 \\
\end{cases}$$
![logistics算法代价函数](images\machine_learning\image6.png)
### 简化代价函数 和 梯度递减法求最优参数  
简化代价函数：  
$J(\theta) = \frac{1}{m}\sum\limits_{i=1}^{m}cost(h_\theta(x)^i,y)$  
$cost(h_\theta(x),y) = -y\cdot log(h_\theta(x)^i)-(1-y)\cdot log(1-h_\theta(x)^i)$
$$\downarrow\downarrow$$
$$J(\theta) = -\frac{1}{m}\sum\limits_{i=1}^{m}[y\cdot log(h_\theta(x)^i)+(1-y)\cdot log(1-h_\theta(x)^i]$$
梯度下降法：  
![梯度下降求最优参数](images\machine_learning\image7.png)
注意这里的$\theta$要同时更新迭代
### 高级优化
可以使用比梯度下降法更高效的高级优化算法来处理不同类型的问题  
| 优化方法     | 是否需梯度 | 是否用Hessian | 优势         | 劣势/限制         |
| -------- | ----- | ---------- | ---------- | ------------- |
| 梯度下降     | √     | ×          | 简单易实现      | 收敛慢，需调学习率     |
| 共轭梯度     | √     | ×          | 收敛比梯度下降快   | 实现稍复杂         |
| BFGS     | √     |  近似      | 收敛快        | 内存高，适用于中小规模问题 |
| L-BFGS   | √     |  近似      | 高效，低内存消耗   | 对非常复杂模型仍有挑战   |
| 牛顿法      | √     | √          | 快速收敛，精度高   | Hessian计算成本高  |
| 动量 / NAG | √     | ×          | 抑制震荡，加速下降  | 需调动量参数        |
| Adam     | √     | ×          | 收敛快，自适应学习率 | 偶尔会欠收敛或过早收敛   |
