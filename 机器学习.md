[toc]
# 机器学习(ML)
T:任务  
E:经验  
性能度量：P  
## 监督学习
人为的对数据有满足任务目标的定性，类似这个目标问题的‘正确答案’（手动完成标记），比如对数据的分类和具体的值  
监督学习常见的分为：  
1. 回归问题：处理连续值  
2. 分类问题：处理间断值  
## 无监督学习（聚类算法）
缺少人为的标记，由算法识别数据内部间的模式与关系。
# 单变量线性回归
## 模型描述  
m = 训练样本的数量  
x = 输入变量（特征）  
y = 输出变量（特征）  
$(x,y)$  一个训练样本  
$(x^{i},y^{i})$ 第i个训练样本
> 监督学习算法的工作流程：  
> ![监督学习算法示意图](/images/image.png)
## 单变量线性回归模型
假设函数：  
$$h_\theta(x)=\theta_0+\theta_1\cdot x$$  
$\theta_i$称为模型参数
### 均方误差代价函数
> 代价函数：  
> 任何能够衡量模型预测出来的值h(θ)与真实值y之间的差异的函数都可以叫做代价函数C(θ)，如果有多个样本，则可以将所有代价函数的取值求均值，记作J(θ)  
> - 对于每种算法来说，代价函数不是唯一的；
> - 代价函数是参数θ的用函数
> - 总的代价函数J(θ)可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本(x,y)；
> - J(θ)是一个标量；  
>理想情况下，当我们取到代价函数J的最小值时，就得到了最优的参数θ  

代价函数：  
$J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits_{i=1}^{m}\left( h_\theta(x)-y\right)^2   $  
优化目标：  
$minimizeJ(\theta_0,\theta_1)$

### 梯度下降法
从给定的点出发，按照一定的学习速率尝试下降速率最大的点，得到局部（或全局）的最小值点  
以代价函数为例：  
$$\theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,\dots)$$  
:= 表示赋值  
$\alpha$ 表示下降学习速率（步长）  
$J(\theta)$ 表示代价函数，$\theta$为参数，可以有多个
在循环到收敛的过程中，要实现多个参数的同步更新


# 多变量线性回归
![多变量线性回归模型描述](\images\image2.png)
n = 变量个数  
m = 样本个数  
$$x^i=\begin{bmatrix}
    x_1^i \\
    x_2^i \\
    \dots
\end{bmatrix}$$  
$x^i$ 表示第i个训练样本  
$x^i_j$ 表示第i个训练样本下的变量值  
## 假设函数
假设函数：  
$$h_\theta(x)=\theta_0+\theta_1\cdot x_1+\theta_2 \cdot x_2+\theta_3\cdot x_3+\dots$$   
也可以写作：  
变量向量$x=\begin{bmatrix}
    x_0 \\
    x_1 \\
    x_2 \\
    x_3 \\
    \dots
\end{bmatrix}\quad(x_0=1)$  
参数向量$\theta = \begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
    \theta_2 \\
    \theta_3 \\
    \dots
\end{bmatrix}$   
$$h_\theta(x) = \theta^T\cdot x $$  

## 代价函数和梯度下降
同一元的回归方程代价函数  
$$J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}\left(h_\theta(x^i)-y\right)^2$$
多元代价函数使用梯度下降法求局部最小值：  
$$ \theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta)$$
注意变量$\theta$同时更新  

### 特征缩放  

当不同特征变量差异很大时，可以考虑使用特征放缩和均值归一化的方法加快运算  
均值归一化：  
$$ x_i' = \frac{x_i-\mu_i}{max(x_i)-min(x_i)}$$  
$\mu_i$是$x_i$的均值
> 也就是把多个变量均值统一为0
  
特征放缩：  
$$x' =\frac{x_i}{max(x_i)-min(x_i)} $$
> 也就是把多个变量的定义域统一到(-1,1)
​
### 调参 和 学习率$\alpha$
如果$\alpha$过小，则收敛速率很慢  
如果$\alpha$过大，则有可能不收敛或者可能出现异常的代价函数J增加  

- 通常通过绘制J和迭代次数的点线图像来判断是否收敛以及确定收敛点    
- 也可以设定阈值$\epsilon$，当$J(\theta)<\epsilon$时，使用此时的收敛点（不太推荐）
## 新特征和多项式回归
![多项式回归](\images\image3.png)
当样本不符合线性关系时，可以考虑使用多项式回归，选取新的特征变量，按照相同的模式：  
> 构建代价函数 -> 梯度下降法选取合适参数

## 正规方程
仅能适用于多元线性回归
![正规方程求解代价函数最小值点](\images\image4.png)
X为设计矩阵，y为因变量矩阵  
设计矩阵：  
若x为一个特征矩阵 $x^i=\begin{bmatrix}
    x_0^i \\
    x_1^i \\
    \dots
\end{bmatrix}$  
设计矩阵X为多个x的转置组成的矩阵
取$X=\begin{bmatrix}
    (x^1)^T \\
    (x^2)^T \\
    \dots   \\
    (x^m)^T    
\end{bmatrix}$
> 使用正规方程求参数时，不需要均值归一化和特征缩放  

