# 基本概念
1. 个体、环境
2. 状态：个体处于环境中，在某时刻对自身的认识  
3. 行为：个体在某时刻对环境施展的行为可以对环境造成改变，同时个体会收到反馈
4. 奖励：个体实施动作后，环境对个人的反馈
5. 价值：个体对未来奖励的预测，衡量个体在某一状态（或采取某个行为组合）的好坏程度并指导后续行为的选择  
   1.  状态价值函数：从状态到价值的映射，评估某一时刻所获得的奖励值  
   2.  行为价值函数：从行为到价值的映射，评估针对某个状态下的某一行为所获得的奖励值  
6. 环境的动力学特征：  
   环境内部的限制和规则，环境不会告知个体其动力学特征，只会提供个体的可观测信息和每个行为后的奖励信息。  
    根据环境对个体提供的观测信息开放程度，从大到小依次分为 **完全可观测、部分可观测**
7. 策略：个体在不同状态下对环境施加的各种不同行为，是个体状态到行为的映射  
    1. 确定性策略：在一个确定的状态下能够产生一个确定的行为  
    2. 随机性策略：在确定状态下不能产生确定行为，只能提供采取 各种行为的概率  
8. 个体的分类： 
   1. 是否有策略函数、价值函数 
      1. 仅基于价值函数：有价值状态的估计函数，没有直接的策略函数，策略由价值函数间接决定  
      2. 仅直接基于策略函数：行为直接由策略函数产生，不维护对状态价值的估计函数  
      3. 演员-批评家类型：既有价值函数也有策略函数
   2. 是否建立针对环境动力学的模型  
      1. 不基于模型：不试图了解环境的工作，仅关注价值或策略函数  
      2. 基于模型：尝试建立一个描述环境运作过程的模型，借此指导价值或策略函数更新    
9.  学习：个体与环境交互，逐渐改善其行为的过程  
10. 规划：个体对环境如何工作有一定认识，进行交互前模拟分析交互情况的过程  
11. 探索：尝试当前状态下的非最优解，可能发现潜在的更优状态  
12. 利用：使用当前状态下的最优解，在当前最优解的基础上进行下一步探索  
13. 预测：求解给定策略下的价值函数问题，在给定的策略下，个体对未来的评价  
14. 控制：试图寻找一个最优策略来最大化奖励  
15. 深度强化学习DRL：将深度学习相关算法融入强化学习算法中，可以节省计算消耗和内存使用，解决大规模的实际问题  

# 马尔可夫决策过程MDP
## 马尔可夫过程MP
由元组<S,P>组成，S为有限数量的状态集，P为不同状态间的状态转移概率矩阵  
$P_{ss'}=P[S_{t+1}=s'|S_t=s]$  
采样：从符合马尔可夫给定的状态转移概率矩阵中生成一个状态序列的过程  
## 马尔可夫奖励过程MRP
由元组$<S,P,R,\gamma>$构成  
S:有限状态集  
P:集合中状态转移概率矩阵 $P_{s}=P[S_{t+1}=s'|S_t=s]$    
R:奖励函数 $R_s=\mathbb{E} [R_{t+1}|S_t=s]$  
$\gamma$:衰减因子，$\gamma \in [0,1]$  
每到一个状态下，都有其对应的奖励函数$R(S_t)$  
收获（回报）：从采样开始到结束的奖励的衰减之和
$$G_t = R_{t+1}+\gamma R_{t+2}+ \gamma^2R_{t+3}+\dotsb $$
价值：马尔可夫奖励过程中状态收获的期望
$$\begin{align*}
    \nu(s)&=\mathbb{E}[G_t|S_t=s] \\
    &=\mathbb{E}[R_t+\gamma\nu(S_{t+1})|S_t=s] \\
    &=R_s+\gamma\sum_{s'\in S}P_{ss'}\nu(s')        （贝尔曼方程）
\end{align*}$$
贝尔曼方程：一个状态的价值是由当前状态的实施奖励$R_s$和衰减后的所有可能的下一个的价值$\nu(s')$决定的
$$\begin{align*}
    \nu&=R+\gamma P\nu \\
    \nu &=(1-\gamma P)^{-1}R
\end{align*}$$
该矩阵方程可以直接计算各状态的价值，但时间复杂度较高$(O(n^3) )$  

## 马尔可夫决策过程MDP
马尔可夫决策过程是在已有的奖励过程的基础上，叠加了执行行动的相同马尔科夫链  
由元组$<S,A,P,R,\gamma>$构成  
S:有限状态集  
A:有限行为集  
P:基于行为的状态转移概率矩阵 $P^a_{ss'}=P[S_{t+1}=s'|S_t=s,A=a]$  
R:基于状态和行为的奖励函数 $R_s=\mathbb{E} [R_{t+1}|S_t=s，A=a]$  
$\gamma$:衰减因子，$\gamma \in [0,1]$
$\pi$:某一状态下采取某一行为的概率（策略） $\pi(a|s)=P[A_t=a|S_t=s]$  
行为价值函数：在s情况下采取了a行为的价值函数
$$\begin{align*}
    q_\pi(s,a)&=\mathbb{E}[G_t|S_t=s,A_t=a] \\
    &=\mathbb{E}[R_t+1+\gamma q_\pi(S_{t+1,A_{t+1}})|S_t=s,A_t=a]   \\
    &= R_s^a+\gamma\sum_{s'\in S} P^a_{ss'}\nu_\pi(s')
\end{align*}$$
价值函数：基于策略$\pi$的状态价值函数
$$\begin{align*}
    \nu_pi(s)&= \mathbb{E}[G_t|S_t] \\
    &= \mathbb{E}[R_{t+1}+\gamma\nu(S_{t+1})|S_t=s]    \\
    &= \sum_{a\in A}\pi(a|s)q_\pi(s,a)  \\
    &= \sum_{a\in A}\pi(a|s)\left(R_s^a+\gamma\sum_{s'\in S} P^a_{ss'}\nu_\pi(s')\right)
\end{align*}$$